---
title: "LiteLLM のロードバランシングをローカルで試してみた"
emoji: "🎉"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LiteLLM", "GenerativeAI"]
published: true
---
こんにちは。  
ご機嫌いかがでしょうか。  
"No human labor is no human error" が大好きな[吉井 亮](https://twitter.com/YoshiiRyo1)です。  

LiteLLM を試してみます。  
Coding Agent 無しには仕事ができないこの頃ですが、モデルどうするんだ問題があると思っています。  
LiteLLM の可能性を探っていきます。  

## OpenAI

OpenAI のキーを発行します。個人でのローカルキー管理が完璧であることが前提です。  

[ここ](https://platform.openai.com/login)から OpenAI にログインし、API キーを発行します。  

## Google Cloud

サービスアカウントのキーを発行します。個人でのローカルキー管理が完璧であることが前提です。  

Google Cloud プロジェクトを開き、「API とサービス」 -> 「Vertex AI API」からサービスアカウントを作成します。  
権限は「Vertex AI ユーザー」を付与します。  
その後サービスアカウントのキーを JSON 形式でダウンロードします。  

## AWS

アクセスキーを発行します。個人でのローカルキー管理が完璧であることが前提です。  

IAM ユーザーを作成します。ポリシーは「AmazonBedrockLimitedAccess」を付与します。  
作成したユーザーの「セキュリティ認証情報」からアクセスキーを作成し、アクセスキー ID とシークレットアクセスキーをダウンロードします。  

さらに [使いたいモデルを有効](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) にしてください。  

## Docker Compose Configuration

ローカルでは Docker Compose を使用して LiteLLM を起動することにします。  
起動手順は [BerriAI/litellm](https://github.com/BerriAI/litellm/tree/main) に記載があります。これを参考にします。  
あくまでローカル起動前提です。インターネットに面した場所で起動しないようご注意ください。  

```yaml:docker-compose.yaml
services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    env_file: .env
    ports:
      - "4000:4000"
    environment:
      DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
      STORE_MODEL_IN_DB: "True"
    volumes:
      - ./config.yaml:/app/config.yaml
      - ./vertex-ai-key.json:/app/vertex-ai-key.json
    command:
      - "--config=/app/config.yaml"

  db:
    image: postgres:16
    restart: always
    container_name: litellm_db
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: "redis:latest"
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    environment:
      - REDIS_PASSWORD=redispassword6379

volumes:
  prometheus_data:
    driver: local
  postgres_data:
    name: litellm_postgres_data
  redis_data:
    name: litellm_redis_data
```

## 環境変数の設定

`.env` ファイルを作成します。値はご自身の環境に合わせて設定してください。  

```env:.env
# Litellm Environment Variables
LITELLM_MASTER_KEY="your-master-key"
LITELLM_SALT_KEY="your-salt-key"

# OpenAI API Key
OPENAI_API_KEY="your-openai-api-key"

# Google Cloud Platform Credentials
GOOGLE_APPLICATION_CREDENTIALS="/app/vertex-ai-key.json"
GOOGLE_PROJECT_ID="your-gcp-project-id"

# AWS Credentials
AWS_ACCESS_KEY_ID="your-aws-access-key-id"
AWS_SECRET_ACCESS_KEY="your-aws-secret-access-key"
AWS_REGION_NAME="your-aws-region"
```

## LiteLLM 設定

LiteLLM 設定を行います。  

`config.yaml` ファイルを作成します。  
OpenAI、Google Cloud、AWS から複数のモデルを設定しています。  
後の手順で Load Balancing を試すので、その設定も入れています。  

- Load Balancing
    - Router Load Balancing
        - default
        	- gpt-4.1-mini
			- gemini-2.5-flash-lite
			- us.amazon.nova-pro-v1:0
	- Proxy Load Balancing
		- nova-micro
			- us.amazon.nova-micro-v1:0
			- apac.amazon.nova-micro-v1:0

```yaml:config.yaml
model_list:
  - model_name: gpt-4.1
    litellm_params:
      model: openai/gpt-4.1
      api_key: os.environ/OPENAI_API_KEY
  # Router Load Balancing
  - model_name: default
    litellm_params:
      model: openai/gpt-4.1-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: default
    litellm_params:
      model: vertex_ai/gemini-2.5-flash-lite
      vertex_ai_project: "your-gcp-project-id"
      vertex_credentials: os.environ/GOOGLE_APPLICATION_CREDENTIALS
      vertex_location: "us-central1"
  - model_name: default
    litellm_params:
      model: bedrock/us.amazon.nova-pro-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: os.environ/AWS_REGION_NAME
  # Proxy Load Balancing
  - model_name: nova-micro
    litellm_params:
      model: bedrock/us.amazon.nova-micro-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: "us-west-2"
  - model_name: nova-micro
    litellm_params:
      model: bedrock/apac.amazon.nova-micro-v1:0
      aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID
      aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY
      aws_region_name: "ap-northeast-1"

general_settings: 
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL

router_settings:
  timeout: 10
  redis_host: redis
  redis_password: os.environ/REDIS_PASSWORD
  redis_port: 6379
```

[Config.yaml Overview](https://docs.litellm.ai/docs/proxy/configs)  

## LiteLLM の起動

Docker Compose を使用して LiteLLM を起動します。以下のコマンドを実行してください。  

```bash
docker compose up -d
```

## Router - Load Balancing

[Router Load Balancing](https://docs.litellm.ai/docs/routing) は複数のモデルプロバイダー間をまたがるバランシングです。  
今回だと OpenAI、Google Cloud、AWS のモデルをバランスします。  
`config.yaml` は上を見てください。`model_name` が `default` になっている箇所が3つあります。  
その下に複数のモデルを定義しています。これが Router Load Balancing の設定です。  

- Router Load Balancing
    - default
    	- gpt-4.1-mini
        - gemini-2.5-flash-lite
        - us.amazon.nova-pro-v1:0

対障害機能、アラート、キャッシングといった本番運用に必要な機能も備えていますが、今回はシンプルなバランシングです。  

docker compose で起動した LiteLLM に対して、以下のコマンドで Router Load Balancing のテストを実施します。  
何回か実行した結果を抜粋して貼り付けています。レスポンスから `jq .model` でモデル名を抽出しています。ご覧の通り、3つのモデルが返ってきています。成功です。  

```bash
# 1
curl -X POST 'http://localhost:4000/chat/completions'   -H 'Authorization: Bearer sk-1234'   --header 'Content-Type: application/json'   --data '{
    "model": "default",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ]
}' | jq .model

"us.amazon.nova-pro-v1:0"

# 2
curl -X POST 'http://localhost:4000/chat/completions'   -H 'Authorization: Bearer sk-1234'   --header 'Content-Type: application/json'   --data '{
    "model": "default",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ]
}' | jq .model

"gpt-4.1-mini-2025-04-14"

# 3
curl -X POST 'http://localhost:4000/chat/completions'   -H 'Authorization: Bearer sk-1234'   --header 'Content-Type: application/json'   --data '{
    "model": "default",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ]
}' | jq .model

"gemini-2.5-flash-lite"
```

## Proxy - Load Balancing

[Proxy Load Balancing](https://docs.litellm.ai/docs/proxy/load_balancing) は、同じモデルを複数のインスタンスでバランシングします。  
「インスタンス」の意味ですが、今回は異なるリージョンを指してします。試してないですが、AWS アカウントを分けることもできると思います。（いつか試します）  

今回の例だと、Amazon Nova Micro モデルを2つのリージョンでバランスします。  

- Proxy Load Balancing
	- nova-micro
		- us.amazon.nova-micro-v1:0
		- apac.amazon.nova-micro-v1:0

テスト結果の抜粋を貼り付けています。  
`apac.amazon.nova-micro-v1:0` と `us.amazon.nova-micro-v1:0` の2つのモデルが返ってきています。成功です。  

```bash
# 1
curl -X POST 'http://localhost:4000/chat/completions'   -H 'Authorization: Bearer sk-1234'   --header 'Content-Type: application/json'   --data '{
    "model": "nova-micro",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ]
}' | jq .model

"apac.amazon.nova-micro-v1:0"

# 2
curl -X POST 'http://localhost:4000/chat/completions'   -H 'Authorization: Bearer sk-1234'   --header 'Content-Type: application/json'   --data '{
    "model": "nova-micro",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ]
}' | jq .model

"us.amazon.nova-micro-v1:0"
```

## Claude Code

さて、いよいよお待ちかねの Claude Code を LiteLLM 経由で使ってみます。  
手順は [tutorial](https://docs.litellm.ai/docs/tutorials/claude_responses_api) に記載されています。  

まずはキーを発行します。以下のコマンドの `sk-1234` の部分はご自身のキーに置き換えてください。（`.env` ファイルに記載されています）  

```bash
curl -L -X POST 'http://localhost:4000/key/generate' \
-H 'Authorization: Bearer sk-1234' \
-H 'Content-Type: application/json' \
-d '{}' | jq .
```

キーが発行されたら、環境変数に設定します。  

```bash
export ANTHROPIC_AUTH_TOKEN="your-generated-key"   # 1つ前のコマンド結果から取得
export ANTHROPIC_BASE_URL="http://localhost:4000"
```

Claude Code を起動します。`--model default` オプションを付けます。  
Router Load Balancing の `default` モデルを指定しています。  

```bash
claude --model default
╭───────────────────────────────────────────────────╮
│ ✻ Welcome to Claude Code!                         │
│                                                   │
│   /help for help, /status for your current setup  │
│                                                   │
│   cwd: /Users/ryo.yoshii/sandbox/litellm          │
│                                                   │
│   ─────────────────────────────────────────────── │
│                                                   │
│   Overrides (via env):                            │
│                                                   │
│   • API Base URL: http://localhost:4000           │
╰───────────────────────────────────────────────────╯

 ※ Tip: Use /agents to create context-efficient experts for specific tasks. Eg. Code Reviewer, Software Architect, Data Scientist

> hello

⏺ Hello! How can I assist you today?
```

無事に会話できました。LiteLLM 経由でモデルが呼び出されています。  
現場からは以上です。  

## 今後

ローカルでの起動でしたが、クラウド上でホストして複数人で使用できるような設計構築を考えています。  
Coding Agent を使う人達に生のクレデンシャルを配るよりかはセキュアになると期待しています。  
また、モデルのリソース枯渇問題もありますので、これも解決していければと思います。  

## 参考

https://docs.litellm.ai/docs/  

https://docs.litellm.ai/docs/tutorials/claude_responses_api  

https://docs.anthropic.com/ja/docs/claude-code/llm-gateway
